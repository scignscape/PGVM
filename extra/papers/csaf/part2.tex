
\part{Interoperating with Data Sets and Scientific Repositories}

\p{Systematic use of published scientific data requires 
accessing information on several different levels, 
including outlines of research methods and protocols 
as well as raw research data.  Several standard 
models exist for how research data sets 
should be described, including Research Objects\footnote{See 
\bhref{https://eprints.soton.ac.uk/271587/1/research-objects-final.pdf}}, 
the \FAIR{} (Findable, Accessible, 
Interoperable, Reusable) initiative\footnote{See \bhref{https://www.researchgate.net/publication/331775411_FAIRness_in_Biomedical_Data_Discovery}.}, 
\SciXML{},\footnote{See \bhref{http://able.myspecies.info/scixml} } 
and \SciData{}\footnote{See \bhref{https://stuchalk.github.io/scidata/}.}.  
Also relevant to scientific data sharing, though less general 
in scope, are formats for composing and/or annotaging scientific 
publications, such as 
\IeXML{}\footnote{See \bhref{https://www.semanticscholar.org/paper/IeXML\%3A-towards-an-annotation-framework-for-semantic-Rebholz-Schuhmann-Kirsch/1d72a56b6576117c62f388a5f2193965e4c7e293}.} and \JATS{} (Journal Article Tag Suite).  Despite (or perhaps beause of) this diversity of 
representational possibilities, guidelines for construsting 
and annotating scentific data remain open-ended: researchers 
who want to rigorously annotate/document their published 
data can do so in many different ways, which 
presents challenges to software that interacts with research 
data.}

\p{Several different kinds of applications interface with 
scientific data, including software where such data 
is \textit{created}; web services (and potentially \API{}s) where 
data is \textit{hosted}; and programs which researchers 
use to find and download (or otherwise access) scientific 
data.  Because data-representation paradigms vary, 
these applications need to be flexible enough to 
manipulate data and metadata packages structured in 
divergent ways.  For example, each \MdsX{} module 
is designed around a particular scientific 
repository/portal's protocols, so the structure 
of distinct \MdsX{} modules may be noticeably 
different.  Nevertheless, some general structuring 
principles can be developed with respect 
to querying and consuming repositories' resources.  
This motivates the idea of a general 
\q{Scientific Data Repository Model,} 
(\SDRM{}) which 
is concretized within code implementing 
clients for individual portals (e.g., 
\MdsX{} modules).  The following section 
will present an overview of this 
\SDRM{} representation, and subsequent 
sections will specify how \SDRM{} 
is related to \ConceptsDB{} and other 
technologies discussed here.}

\section{Interoperating with Science Portals via 
the Scientific Data Repository Model}

\p{According to the \SDRM{} representation, each 
data set is accompanied/constituted by a collection 
of \q{assets.}  These include raw data files; 
publication texts; research method descriptions; 
code for accessing/examining/analyzing research 
data; and annotations defining interconnections 
between these different elements.  
All material within or associated with a data set 
is classified into one of \SDRM{} \q{units,} including 
the five just enumerated and several 
others.\footnote{For instance, data in some fields --- such 
as genomics --- sometimes becomes too large to 
be conveniently downloaded to an ordinary computer; 
in this case, portals such as GenBank offer query services 
to restrict vary large data sets into small result-sets 
that researchers can examine directly.  In these 
cases, the steps required to obtain information via 
large, remotely-hosted data steps would potentially be 
encapsulated into a distinct \sSDRM{} unit classified 
outside the main \sSDRM{} categories.  Another 
potential example of \sSDRM{} would be 
statements related to funding, potential conflicts 
of interest, and so forth.}  
\lSDRM{} units represent both serializations of 
data-set information, as well as \GUI{} components 
(such as Dialog Boxes) allowing readers of 
publicatons (and users of corresponding data sets) 
to view the unit's information.   Moreover, 
\SDRM{} units, by design, encapsulate their 
data in type-instances (e.g., objects of 
\Cpp{} classes) that can be manipulated at 
runtime by computer code.  In short, each 
unit has at least three representations 
(serial, \GUI{}, runtime), giving rise to a 
spectrum of \SDRM{} data that can be 
summarized in a table (see Table~\hyperref[tab:sdrm]{1}).}  

\begin{table}
\renewcommand{\arraystretch}{2}
\label{tab:sdrm}
\begin{scriptsize}
\begin{center}
\begin{tabular}{|p{.1\textwidth}|p{.25\textwidth}|p{.25\textwidth}|p{.2\textwidth}|}
\multicolumn{1}{c}{\textbf{\sSDRM{} Unit}} &%
\multicolumn{1}{c}{\textbf{Runtime Object}} &% 
\multicolumn{1}{c}{\textbf{Serialization}} &%
\multicolumn{1}{c}{\textbf{\sGUI{} Component}} \\%
\midrule
\RaggedRight{}Raw Data Files &\RaggedRight{} File and archive objects identifying file types, 
preferred applications, software prerequisites, etc. 
&\RaggedRight{} Structured review of archive layout (e.g., the Research Object 
Bundle \textbf{manifest} file) 
&\RaggedRight{} Filesystem archive view plus information on file types and sizes  \\%
Publication &\RaggedRight{} Machine-Readable Text (e.g., \sHTXN{}) 
&\RaggedRight{}  
Machine-Readable Serializaton (e.g., \sHTXN{}-compatible \LaTeX{}) 
&\RaggedRight{} \sPDF{} Viewer  \\%
\RaggedRight{}Research Method &\RaggedRight{} Research Protocol Encoding (e.g., \sBioCoder{}) 
&\RaggedRight{} Research Protocol Serialization (e.g., an \sMIBBI{}-based markup language) 
&\RaggedRight{} Dialog Box to view \q{Minimum Information} checklist   \\
\midrule
\RaggedRight{}Dataset-Specific Code Library &\RaggedRight{} Descriptions of library dependencies, compile 
steps, inter-type relations (e.g. corresponences between data object and 
\sGUI{} types), etc. 
&\RaggedRight{} Serialized build information  
&\RaggedRight{} Dialog to set compile/user options; \sIDE{} integration  \\
\midrule
\RaggedRight{}Annotations on Data, Code, and Text &\RaggedRight{} Integrated Annotation Object 
Model &\RaggedRight{} Annotation Encoding (e.g. LTS's \q{Annotation Exchange Format}   
&\RaggedRight{} Annotation-related context menus within \sPDF{} viewers and 
dataset \sGUI{} elements 
\bottomrule
\end{tabular}
\end{center}
\end{scriptsize}
\caption{Representations for \SDRM{} Units}
\end{table}

\p{In general, the data within \SDRM{} units might be 
obtained from different sources, depending on the 
module --- e.g., from publishers' \API{}s, or 
from downloaded data sets, or some combination.  
It is better to make at least part of the \SDRM{} data available 
without having to download data sets as a whole, because 
a useful overview of the data set may factor 
in whether users choose to download it to begin with.} 
 
\p{When \SDRM{} information is provided as part of a 
dataset (rather than indirectly through an \API{} or 
some other remotely-accessed resource), the \SDRM{} \GUI{} 
components can be integrated with the data set's 
overall front-end.  In particular, if the data set 
provides its own, customized \GUI{} code 
--- yielding what LTS calls a \q{Data-Set Application} 
--- then such \GUI{}s may include dialog boxes 
and similar components representing \SDRM{} data.  
Data-Set Applications (which may be implemented 
with the aid of \MdsX{} libraries) are discussed in 
the next section.}

\section{Data-Set Applications}

\p{Dataset Creator (\dsC{}) is a framework for 
constructing data sets which include computer code based on the 
\Qt{} application-development platform.  
Dataset Creator takes advantage of the \Qt{} platform 
to construct Research Objects with exceptional 
\GUI{} and data-mining capabilities.  \lQt{}, the 
leading native cross-platform development toolkit, 
is a comprehensive framework encompassing 
a thorough inventory of programming features 
--- networking, \GUI{} implementation, file 
management, data visualization, \ThreeD{} graphics, 
and so forth.  Data sets based on \Qt{} require 
users to obtain a copy of the \Qt{} platform, but 
\Qt{} is free for non-commercial use and easy to 
install --- importantly, \Qt{} is wholly contained 
in its own folder and does not affect any other 
files on the user's computers (in this manner \Qt{} is 
different than most software packages, which usually 
demand a \q{system install}).}

\p{By leveraging the \Qt{} platform, \dsC{} enables 
standalone, self-contained, and full-featured 
native/ desktop applications to be uniquely implemented 
for each data set, distributed in source-code fashion 
along with raw research data.  Adopting such a 
data-curation method makes data sets easier to 
use across a wide range of scientific disciplines, 
because the data sets are freed from having to rely on domain-specific 
software (software which may be commonly used in one scientific 
field but is unfamiliar outside that field).  
In addition, Research Objects composed with \dsC{} can 
be integrated into Multi-Application Networks  
(which are described above) because 
the dataset applications are autonomous native \GUI{} applications that 
can easily interoperate via \Qt{} messaging protocols.}

\p{Because every data set is unique, each Dataset Application 
will necessarily include some code specific to that one 
Research Object.  However, \dsC{} will provide a core 
code base and file layout which is shared by all 
\dsC{} data sets by default.  This common core is 
structured in part by the goal of developing 
Dataset Applications in a \Qt{} context; for 
instance, \dsC{} projects are structured to use \Qt{}'s \q{qmake} 
build system as the primary tool for compiling 
data-set code.  The common \dsC{} code therefore 
includes qmake project files which support 
compiling application with several build configurations.  
In this framework, data-set users are classified into several 
different roles --- in addition to ordinary users 
(specifically, researchers who want to work with 
and draw information from 
data sets but have no development connection to 
these data sets themselves), \dsC{} recognizes 
roles for authors, editors, testers, and other 
users who are responsible for bringing data 
sets into publication-ready form to begin with.  
Depending on the administrative role, data set 
code can be compiled with additional 
features (e.g., unit testing features).}

\p{Another core component of \dsC{} is \LaTeX{} 
code that authors may use when preparing documents 
accompanying their data set.  These \LaTeX{} files 
encompass special functionality for defining code 
annotations and semantically significant points in 
article text, such as sentence and paragraph 
boundaries.  This \LaTeX{} code can be used 
in conjunction with a pre-processor that 
generates \LaTeX{} files from a special input 
language.  The goal of these text-processing 
technologies is to improve the interoperability 
between research papers and data sets.  In particular, 
the \LaTeX{} pre-processors (and subsequent 
\LaTeX{}-to-\PDF{} converters) generate \HGXF{} 
files which store information about textual and 
\PDF{} viewport coordinates specifying the location of semantically 
meaningful elements such as sentences and annotations.  
These files are then zipped and embedded in 
\PDF{} files.  With \dsC{}, the resulting \PDF{} files can 
then be loaded into a customized \PDF{} viewer capable of 
reading the embedded \HGXF{} data.  This allows the 
\PDF{} application to utilize the embedded information 
so as to provide a more interactive reading 
experience --- for instance, viewing annotations 
or copying sentences via context menus, where viewport 
data maps cursor position to textual elements visible 
on the \PDF{} page.  These features provide an 
application-level interface between the \PDF{} viewers 
(considered as \GUI{} components) and the corresponding 
\GUI{} components in Dataset Applications.}

\p{With proper customization, both the \PDF{} viewers 
and the \dsC{} Dataset Applications can interoperate, 
with \PDF{} context menus calling up windows in 
the Dataset Application's \GUI{} implementation, 
and vice-versa.  For example, researchers reading 
the \PDF{} version of a scientific article can 
launch the Dataset Application to explore some 
detail mentioned in the text.  This is an example of 
where micro-citations are practically useful: 
any microcitable element in a document (such as a table, 
column, row/record, or analytic procedure formalized as a 
procedural asset associated with a data set) 
can be linked to a corresponding \GUI{} element 
in the Dataset Application.  For example, a statistical 
parameter --- mentioned by name in the text, and perhaps 
represented in serialization within raw data --- can 
be mapped to a \GUI{} table column, and specifically 
the column header; this is then an annotation target, 
in the sense that for readers to gain more information it 
is proper to link mentions of the relevant scientific 
concept in article text to the column header as a 
graphical element that can be made visible.  The 
link is operationalized by implementing a procedure 
to show the \GUI{} window where the table is located, 
and ensure that the column header lies in the visible 
portion of the screen, as a response to readers on the 
\PDF{} side signaling a desire for information on 
the annotated text element.  In the opposite direction, 
database elements can be annotated with 
links that can used by the Dataset Application to launch 
a \PDF{} window opened to the page and location where 
the corresponding concept is discussed in the article.}

\p{In order to properly model this semantic, 
viewport, and data set data integration, 
\dsC{} uses \HTXN{}, described earlier, for document representation.  
With \dsC{}, \HTXN{} files are not only associated 
with data set assets; they are also machine-readable 
document encodings that can be introduced into 
publication repositories and other corpora 
oriented toward machine-readability.  Authors can 
host \HTXN{} files within data sets and link to 
them via services such as CrossRef, thereby 
ensuring that a highly structured, machine-readable 
version of their papers is available for 
text and data mining.  The \HTXN{} protocol is 
also useful for encoding natural-language content 
which becomes part of a data set as data assets 
in themselves; for example, linguistic case-studies, or 
(in the biomedical context) patient narratives.} 

\p{In order to establish annotation cross-references 
between publications and data-set code, \dsC{} 
provides a code-annotation system based 
on hypergraphs.  Further documentation of hypergraph 
structuring principles and how they apply to 
representing computer code can be found 
in the LTS paper reviewing \ConceptsDB{}.}

%\p{}


